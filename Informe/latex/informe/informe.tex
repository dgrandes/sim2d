\documentclass[12pt]{article}

\usepackage[latin1]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[a4paper,left=3cm,right=3cm]{geometry}
\usepackage{graphicx}

\title{Proyecto Final: Simulador de Muchedumbres}
\date{Junio, 2012}


\begin{document}

\author{
	David Grandes\\
       Instituto Tecnológico de Buenos Aires\\
       dgrandes@alu.itba.edu.ar
\and   
	Matías Pan\\
       Instituto Tecnológico de Buenos Aires\\
       mpan@alu.itba.edu.ar
}

\maketitle

\pagebreak

\section{Resumen}

Este proyecto tiene como objetivo la realización de un Simulador de Muchedumbres que permita la investigación de distintas políticas de movimiento. Se investigará el desempeño de técnicas de Aprendizaje Reforzado en la navegación de los agentes en distintos entornos.

La modelización y simulación de muchedumbres es un problema de interés para varias disciplinas de ingeniería y logística. Este Proyecto Final de Grado tiene como objetivo la investigación de Aprendizaje por Refuerzo como política en la simulación de muchedumbres, con el deseo de encontrar una manera más realista de simular la navegación/elusión de obstáculos fijos o móviles (otros peatones).

La investigación de la efectividad de políticas de Aprendizaje con Refuerzo se realizó a través de un simulador. El simulador fue implementado por el equipo y permitió la creación de entornos y distintos tipo de agentes para evaluar el aprendizaje. A su vez el simulador graba las simulaciones y las reproduce posteriormente para poder analizar los resultados.

Los resultados a favor del aprendizaje con Refuerzo son variados. Se observan mejoras en ciertos entornos simples, pero en otros mas complejos, donde el agente debe interactuar con alta densidad de obstáculos se nota una tendencia a sobrecompensar y fallar repetidamente al intentar esquivarlos.

\pagebreak

\tableofcontents

\pagebreak

\section{Introducción}

En la actualidad existen diversos métodos que simulan el comportamiento de agentes independientes moviéndose en un entorno. Uno de los mecanismos más sencillos es el uso de fuerzas, atractivas o repulsivas, que gobiernen el movimiento de los agentes. Este mecanismo tiene serias deficiencias en entornos complejos, o con caminos no directos entre el agente y su destino. Si el punto de deseo del agente se encuentra detrás de una pared la fuerza normal de la misma lo repele, dejando al agente atrapado en el lugar donde la fuerza resultante es 0.

Lo que se propone es modificar este modelo por uno de aprendizaje reforzado. El aprendizaje por refuerzo es un área de estudio que contempla como un agente toma decisiones en un entorno no totalmente conocido. El agente maximiza la utilidad mediante refuerzos, dependiendo si tomó una buena o mala decisión. A su vez el agente tiene que encontrar un balance apropiado entre la exploración de mejores resultados y la explotación de conocimiento ya establecido.

Se abordará el problema utilizando el método de aprendizaje por refuerzo llamado Q-Learning. Este método consiste en aprender asignando diferentes utilidades. El agente cambia de un estado a otro realizando una acción a $\in$ A. El algoritmo define la función que define la calidad, la cual el agente intentará maximizar a la hora de tomar decisiones, como:

\begin{equation}
\label{eq-qlearning1}
Q = S \times A \rightarrow R
\end{equation}

Se entrenará utilizando aprendizaje por refuerzo en 3 entornos diferentes y luego se analizará el comportamiento en un entorno donde no podrá aprender sino que deberá explotar el conocimiento ya adquirido. Los agentes que no utilicen Q-Learning se regirán por el modelo de fuerza social.\cite{helbing:panic}

\section{Antecedentes}
\subsection{Modelo de Fuerza Social}

El modelo de fuerza social \cite{helbing:panic} consiste en modelar el comportamiento como una combinación de fuerzas sociológicas y físicas:

\begin{itemize}
\item Fuerza de Deseo 
\item Fuerza Social   
\item Fuerza Granular o de Contacto
\end{itemize}

La fuerza de deseo es el comportamiento natural del agente, que es de ir a donde el desea. La fuerza social representa la tendencia que presentan las personas a permaner alejadas de las demás. La fuerza granular o de contacto son fuerzas que se manifiestan durante el contacto entre individuos u obstáculos.

La ecuación general de movimiento del agente es la siguiente:
\begin{equation}
m_{i}\dfrac{d \mathbf{v}_{i}}{dt} = m_{i} \frac{
	v^{0}_{i}(t) \mathbf{e}_{i}^{0}(t) - \mathbf{v}_{i}(t)
}
{
\mathbf{\tau}_{i}
}
+ \sum_{j(\ne i)}\mathbf{f}_{ij} 
+ \sum_{W}\mathbf{f}_{iW}
\end{equation}

donde $\mathbf{f}_{ij}$ son las fuerzas entre los agentes y $\mathbf{f}_{iW}$ son las fuerzas entre el agente y los obstáculos.

El primer término de la ecuación es la \textit{fuerza de deseo} del agente, donde $\mathbf{e}_{i}^{0}$ es el vector de dirección al punto de interés. $v_{i}^{0}$ es la velocidad de deseo del agente que es modificada por la velocidad actual $\mathbf{v}_{i}(t)$. $\tau_{i}$ representa el tiempo de reacción o demora que tiene el agente en cambiar su velocidad.  

Las fuerzas entre agentes y obstaculos tienen dos componentes, una es la \textit{fuerza social} y la \textit{fuerza granular}.
Definimos las fuerzas entre los agentes ($\mathbf{f}_{i}$) de la siguiente manera:
\begin{equation}
\mathbf{f}_{ij} = \left \{ A_{i}\; \exp \left [ \dfrac{r_{ij} - d_{ij}}{B_{i}}\right ] 
+ 
\kappa g(r_{ij} - d_{ij})
\right \}\mathbf{n}_{ij}
+
\kappa g(r_{ij} - d_{ij}) \Delta v_{ij}^{t}\mathbf{t}_{ij}
\end{equation}


La \textit{fuerza social} esta incluida en el primer término de la ecuación:
\begin{equation}
A_{i}\; \exp \left [ \dfrac{r_{ij} - d_{ij}}{B_{i}}\right ] 
\end{equation}

Es una fuerza de repulsión entre ellos que se hace más grande a medida que se acercan. $A_{i}$ y $B_{i}$ son constantes. $d_{ij}\; =\; ||r_{i}-r_{j}||$ y representa la distancia entre los centros de masa de los agentes. El versor $\mathbf{n}_{ij}$ apunta del agente j al agente i, lo cuál hace a esta una fuerza de repulsión. 

La \textit{fuerza granular} o la \textit{fuerza de contacto} es una fuerza que se manifiesta solamente cuando el agente entra en contacto con otros elementos. Esta fuerza tiene dos componentes. La primera es una fuerza de resistencia de un cuerpo al ser comprimido por una fuerza externa. Esta dada por:
\begin{equation}
kg(r_{ij} - d_{ij})\mathbf{n}_{ij}
\end{equation} 
Esta fuerza va en la misma dirección normal que la \textit{fuerza social}. $\kappa$ es una constante. $g()$ es una función escalonada, que es $1$ si los agentes estan en contacto y $0$ si no. Es esta función la que habilita las fuerzas granulares en la ecuación de movimiento. 

La segunda componente es una fuerza que impide el movimiento tangencial relativo. Esta dada por:
\begin{equation}
\kappa g(r_{ij} - d_{ij}) \Delta v_{ij}^{t}\mathbf{t}_{ij}
\end{equation}

Es igual a la anterior componente en magnitud, lo que cambia es el versor o la dirección. $\mathbf{t}_{ij}$ es la dirección tangencial y esta dada por $\mathbf{t}_{ij}\; =\; (-n_{ij},n_{ij})$. $\Delta v_{ij}^{t}$ es la diferencia de velocidad tangencial entre los agentes.

La fuerza entre el agente y los obstaculos, en nuestro caso las paredes, esta dada por:
\begin{equation}
\mathbf{f}_{iW} = \left \{ A_{i}\; \exp \left [ \dfrac{r_{i} - d_{iW}}{B_{i}}\right ] 
+ 
\kappa g(r_{i} - d_{iW})
\right \}\mathbf{n}_{iW}
+
\kappa g(r_{i} - d_{iW})(\mathbf{v}_{i} \cdot \mathbf{t}_{iW})\mathbf{t}_{iW}
\end{equation}

La ecuación es analoga a la de los agentes salvo por la dirección, que en este caso es normal a la superficie de la pared. Podemos ver que la fuerza social esta presente en esta ecuación también. Para la simulación se usaron estas ecuaciones integradas numéricamente con el \textit{Método de Euler}. Se integro con un $dt \; = \; 0.01s$

\subsection{Aprendizaje por Refuerzo}
El aprendizaje por refuerzo está fuertmente inspirado por la psicología conductista y el concepto de enseñar con medio de recompensas las decisiones que toma el individuo.

El model consiste de los siguientes elementos:
\begin{itemize}
\item conjunto de estados $\textit{S}$
\item Un conjunto de acciones $\textit{A}$
\item Reglas de transición entre los estados
\item Reglas que determinen la recompensa para cada transición posible
\item Reglas que determinen qué es lo que un agente observa en un momento dado
\end{itemize}

El aprendizaje por refuerzo consta de dos etapas diferentes:

\begin{itemize}
\item Exploración: Durante esta etapa se toman decisiones con carácter aleatorio que permite expandir el espacio de búsqueda del agente.
\item Explotación: Donde el agente prioriza la maximización de la recompensa. Durante esta etapa el agente tomará decisiones que le reporten la mejor utilidad.
\end{itemize}

\section{Q-Learning}

El método elegido para el aprendizaje por refuerzo es Q-Learning. Q-Learning es una técnica de aprendizaje por refuerzos que se ajusta perfectamente a los requerimientos del simulador ya que no requiere que se modele el sistema completo para que este sea efectivo, en todo momento el agente solo está consciente de lo que le rodea y nada más.
El proceso de decisión del agente esta dado por la siguiente función de actualización:

\begin{equation}
Q(s_{t},a_{t}) \leftarrow Q(s_{t}, a_{t}) + 
	\alpha_{t}(s_{t},a_{t}) 
	\times \left [ 
		R(s_{t}) + 
		\gamma \; max_{\alpha_{t+1}}Q(s_{t+1},a_{t+1}) -
		Q(s_{t},a_{t})
	\right ]
\end{equation}

El valor que se va a actualizar de la tabla $Q(s_{t},s_{t})$ es la suma del valor viejo con una correción basada en la nueva informacion. El término 
$\alpha(s_{t},a_{t})$ es la taza de aprendizaje. Representa hasta que punto la nueva información reemplaza la anterior. Un valor de $0$ significa que el agente no ha aprendido nada, mientras que un valor de $1$ asegura que el agente considera solo la información más reciente.

La recompensa es representada por $R_{t+1}$ en base a la acción tomada $a_{t}$ para el estado $s_{t}$. El factor de descuento $\gamma$, determina la importancia de recompensas futuras. Un factor $0$ significa un agente oportunista que solo considera la recompensa actual, mientras que un valor que se acerque a $1$ hace que el agente se oriente hacia recompensas de largo plazo. Un valor mayor a cero hace que los valores de $Q$ diverga. El factor de descuento se aplica a la mejor acción que se puede tomar. El valor máximo de utilidad de la próxima acción esta dado por:
\begin{equation}
max_{\alpha_{t+1}}Q(s_{t+1},a_{t+1})
\end{equation}

Nuestras simulaciones fueron realizadas con los siguientes parámetros:
\begin{itemize}
\item Factor de Aprendizaje ($\alpha$) = $0.1$
\item Factor de Descuento ($\gamma$) = $0.9$
\end{itemize}

\section{Trabajo Realizado}

\subsection{Simulador}

El simulador desarrollado cuenta con las siguientes características:
\begin{itemize}
\item Configuración de agentes por medio de archivo.
\item Configuración de entorno por medio de archivo.
\item Configuración de frame skip desde GUI. El usuario, al realizar una simulación puede cambiar, si así lo desea, el frame skip del simulador, esto causa que la actualización visual del estado del simulador se realice cada una cantidad mayor de pasos pero provee la ventaja de liberar el procesador para realizar iteraciones de simulación mas rápidamente.
\item Permite guardar simulaciones en formato binario.
\item Permite reproducir simulaciones guardadas en formato binario.
\item Durante la simulación el usuario puede hacer click derecho sobre un agente para ver un panel que se actualiza en tiempo real con la posición y la descomposición de las fuerzas que actúan sobre él.
\item Durante la simulación el usuario puede hacer click para agregar un overlay sobre los agentes que indica la dirección y magnitud de las fuerzas que actúan sobre él.
\item Posibilidad de pausar una simulación.
\item Posibilidad de reiniciar una simulación.
\item El simulador guarda el estado de los agentes que utilizan Q-Learning a archivos de log para su posterior análisis.
\end{itemize}

Se presta soporte para la declaración de agentes de variados diámetros y masas, permitiendo también al usuario que declare que tipo de movimiento deberá seguir el agente cuando este sea parte de una simulación y cual será el punto hacia el cual el agente deberá ir.
Se pueden declarar también obstáculos de formas geométricas arbitrarias.

Finalmente se permite la declaración de sumideros (puntos de llegada o metas) y de generadores (puntos de donde se generan agentes).

El simulador fue desarrollado utilizando \textit{Java} y \textit{Swing}. Consta de una implementación de interfaz que utiliza extensivamente \textit{Worker Threads} para garantizar la responsividad de la misma al mismo tiempo que previene problemas de concurrencia.

Además la simulación de agentes se realiza utilizando \textit{Thread Pools} que permiten paralelizar los cálculos dentro de una misma iteración y mejorar el tiempo que toman las mismas. Se puede asegurar que al paralelizar los cálculos no se producen problemas en cuanto al modelo de fuerzas sociales ni al de Q-Learning ya que se asegura que los threads están sincronizados y no se empieza nunca con el próximo paso de simulación hasta haber completado el actual.

Teniendo en cuenta ciertas dificultades que se encontraron durante el desarrollo, principalmente en el área de GUI, 
consideramos que \textit{Java}, si bien es un lenguaje muy flexible, puede no haber sido la mejor elección sobre la cual 
construir el proyecto.
Una mejor elección podría ser la utilización de un motor de video juegos para poder aprovechar la sincronización de 
threads y las vastas librerías de vectores y física que estos motores poseen. Entre ellos los que más llaman la atención 
son:

\begin{itemize}
\item Cocos2d: (http://cocos2d.org/) es una librería de Python opensource que existe desde febrero de 2008. Su principal 
ventaja sobre la utilización de JAVA es como ya se mencionó, que este motor permite un manejo mas cómodo de la 
graficación. Su otra ventaja es que al estar basada sobre OpenGL provee una interfaz mas fluida al usuario.
\item Unity3D: (http://unity3d.com/) es un motor de vídeo juegos con soporte nativo multiplataforma. Si bien no es 
opensource como Cocos2d, provee con un entorno de edición que facilita tareas típicas que acelera mucho el 
desarrollo. En nuestra opinión esta es la mejor de las 2 opciones.
\end{itemize}

\section{Conclusión}

\bibliographystyle{abbrv}
\bibliography{01}


\end{document}
