\documentclass[12pt]{article}

\usepackage[latin1]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[a4paper,left=3cm,right=3cm]{geometry}
\usepackage{graphicx}

\title{Proyecto Final: Simulador de Muchedumbres}
\date{Junio, 2012}


\begin{document}

\author{
	David Grandes\\
       Instituto Tecnológico de Buenos Aires\\
       dgrandes@alu.itba.edu.ar
\and   
	Matías Pan\\
       Instituto Tecnológico de Buenos Aires\\
       mpan@alu.itba.edu.ar
}

\maketitle

\pagebreak

\section{Resumen}

Este proyecto tiene como objetivo la realización de un Simulador de Muchedumbres que permita la investigación de distintas políticas de movimiento. Se investigará el desempeño de técnicas de Aprendizaje Reforzado en la navegación de los agentes en distintos entornos.

La modelización y simulación de muchedumbres es un problema de interés para varias disciplinas de ingeniería y logística. Este Proyecto Final de Grado tiene como objetivo la investigación de Aprendizaje por Refuerzo como política en la simulación de muchedumbres, con el deseo de encontrar una manera más realista de simular la navegación/elusión de obstáculos fijos o móviles (otros peatones).

La investigación de la efectividad de políticas de Aprendizaje con Refuerzo se realizó a través de un simulador. El simulador fue implementado por el equipo y permitió la creación de entornos y distintos tipo de agentes para evaluar el aprendizaje. A su vez el simulador graba las simulaciones y las reproduce posteriormente para poder analizar los resultados.

Los resultados a favor del aprendizaje con Refuerzo son variados. Se observan mejoras en ciertos entornos simples, pero en otros mas complejos, donde el agente debe interactuar con alta densidad de obstáculos se nota una tendencia a sobrecompensar y fallar repetidamente al intentar esquivarlos.

\pagebreak

\tableofcontents

\pagebreak

\section{Introducción}

En la actualidad existen diversos métodos que simulan el comportamiento de agentes independientes moviéndose en un entorno. Uno de los mecanismos más sencillos es el uso de fuerzas, atractivas o repulsivas, que gobiernen el movimiento de los agentes. Este mecanismo tiene serias deficiencias en entornos complejos, o con caminos no directos entre el agente y su destino. Si el punto de deseo del agente se encuentra detrás de una pared la fuerza normal de la misma lo repele, dejando al agente atrapado en el lugar donde la fuerza resultante es 0.

Lo que se propone es modificar este modelo por uno de aprendizaje reforzado. El aprendizaje por refuerzo es un área de estudio que contempla como un agente toma decisiones en un entorno no totalmente conocido. El agente maximiza la utilidad mediante refuerzos, dependiendo si tomó una buena o mala decisión. A su vez el agente tiene que encontrar un balance apropiado entre la exploración de mejores resultados y la explotación de conocimiento ya establecido.

Se abordará el problema utilizando el método de aprendizaje por refuerzo llamado Q-Learning. Este método consiste en aprender asignando diferentes utilidades. El agente cambia de un estado a otro realizando una acción a $\in$ A. El algoritmo define la función que define la calidad, la cual el agente intentará maximizar a la hora de tomar decisiones, como:

\begin{equation}
\label{eq-qlearning1}
Q = S \times A \rightarrow R
\end{equation}

Se entrenará utilizando aprendizaje por refuerzo en 3 entornos diferentes y luego se analizará el comportamiento en un entorno donde no podrá aprender sino que deberá explotar el conocimiento ya adquirido. Los agentes que no utilicen Q-Learning se regirán por el modelo de fuerza social.\cite{helbing:panic}

\section{Antecedentes}
\subsection{Modelo de Fuerza Social}

El modelo de fuerza social \cite{helbing:panic} consiste en modelar el comportamiento como una combinación de fuerzas sociológicas y físicas:

\begin{itemize}
\item Fuerza de Deseo 
\item Fuerza Social   
\item Fuerza Granular o de Contacto
\end{itemize}

La fuerza de deseo es el comportamiento natural del agente, que es de ir a donde el desea. La fuerza social representa la tendencia que presentan las personas a permaner alejadas de las demás. La fuerza granular o de contacto son fuerzas que se manifiestan durante el contacto entre individuos u obstáculos.

La ecuación general de movimiento del agente es la siguiente:
\begin{equation}
m_{i}\dfrac{d \mathbf{v}_{i}}{dt} = m_{i} \frac{
	v^{0}_{i}(t) \mathbf{e}_{i}^{0}(t) - \mathbf{v}_{i}(t)
}
{
\mathbf{\tau}_{i}
}
+ \sum_{j(\ne i)}\mathbf{f}_{ij} 
+ \sum_{W}\mathbf{f}_{iW}
\end{equation}

donde $\mathbf{f}_{ij}$ son las fuerzas entre los agentes y $\mathbf{f}_{iW}$ son las fuerzas entre el agente y los obstáculos.

\subsubsection{Fuerza de Deseo}
El primer término de la ecuación es la \textit{fuerza de deseo} del agente, donde $\mathbf{e}_{i}^{0}$ es el vector de dirección al punto de interés. $v_{i}^{0}$ es la velocidad de deseo del agente que es modificada por la velocidad actual $\mathbf{v}_{i}(t)$. $\tau_{i}$ representa el tiempo de reacción o demora que tiene el agente en cambiar su velocidad.

\subsubsection{Fuerza Social y Granular}
Las fuerzas entre agentes y obstaculos tienen dos componentes, la \textit{fuerza social} y la \textit{fuerza granular}.
Definimos las fuerzas entre los agentes ($\mathbf{f}_{i}$) de la siguiente manera:
\begin{equation}
\mathbf{f}_{ij} = \left \{ A_{i}\; \exp \left [ \dfrac{r_{ij} - d_{ij}}{B_{i}}\right ] 
+ 
\kappa g(r_{ij} - d_{ij})
\right \}\mathbf{n}_{ij}
+
\kappa g(r_{ij} - d_{ij}) \Delta v_{ij}^{t}\mathbf{t}_{ij}
\end{equation}


La \textit{fuerza social} esta incluida en el primer término de la ecuación:
\begin{equation}
A_{i}\; \exp \left [ \dfrac{r_{ij} - d_{ij}}{B_{i}}\right ] 
\end{equation}

Es una fuerza de repulsión entre ellos que se hace más grande a medida que se acercan. $A_{i}$ y $B_{i}$ son constantes. $d_{ij}\; =\; ||r_{i}-r_{j}||$ y representa la distancia entre los centros de masa de los agentes. El versor $\mathbf{n}_{ij}$ apunta del agente j al agente i, lo cuál hace a esta una fuerza de repulsión. 

La \textit{fuerza granular} o la \textit{fuerza de contacto} es una fuerza que se manifiesta solamente cuando el agente entra en contacto con otros elementos. Esta fuerza tiene dos componentes. La primera es una fuerza de resistencia de un cuerpo al ser comprimido por una fuerza externa. Esta dada por:
\begin{equation}
kg(r_{ij} - d_{ij})\mathbf{n}_{ij}
\end{equation} 
Esta fuerza va en la misma dirección normal que la \textit{fuerza social}. $\kappa$ es una constante. $g()$ es una función escalonada, que es $1$ si los agentes estan en contacto y $0$ si no. Es esta función la que habilita las fuerzas granulares en la ecuación de movimiento. 

La segunda componente es una fuerza que impide el movimiento tangencial relativo. Esta dada por:
\begin{equation}
\kappa g(r_{ij} - d_{ij}) \Delta v_{ij}^{t}\mathbf{t}_{ij}
\end{equation}

Es igual a la anterior componente en magnitud, lo que cambia es el versor o la dirección. $\mathbf{t}_{ij}$ es la dirección tangencial y esta dada por $\mathbf{t}_{ij}\; =\; (-n_{ij},n_{ij})$. $\Delta v_{ij}^{t}$ es la diferencia de velocidad tangencial entre los agentes.

La fuerza entre el agente y los obstaculos, en nuestro caso las paredes, esta dada por:
\begin{equation}
\mathbf{f}_{iW} = \left \{ A_{i}\; \exp \left [ \dfrac{r_{i} - d_{iW}}{B_{i}}\right ] 
+ 
\kappa g(r_{i} - d_{iW})
\right \}\mathbf{n}_{iW}
+
\kappa g(r_{i} - d_{iW})(\mathbf{v}_{i} \cdot \mathbf{t}_{iW})\mathbf{t}_{iW}
\end{equation}

La ecuación es analoga a la de los agentes salvo por la dirección, que en este caso es normal a la superficie de la pared. Podemos ver que la fuerza social esta presente en esta ecuación también. 

\subsubsection{Desventajas}

La principal desventaja del model de fuerza social, y el motivo por el cúal se busca una alternativa a este, es que los agentes tienden a atascarse cuando se enfrentan a un obstáculo que yace en la dirección al objetivo. Se pretende solucionar este problema permitiendo al agente ``esquivar'' obstáculos antes de llegar a la situación de contacto.

\begin{figure}
\centering
\includegraphics{images/fuerzasocial.png}
\caption[Fuerza Social]{\small{Situaciones problemáticas para el modelo de fuerza social}}
\label{figProbFuerzaSocial}
\end{figure}

\subsection{Aprendizaje por Refuerzo}
El aprendizaje por refuerzo está fuertemente inspirado por la psicología conductista y el concepto de enseñar con medio de recompensas las decisiones que toma el individuo.

El model consiste de los siguientes elementos:
\begin{itemize}
\item conjunto de estados $\textit{S}$
\item Un conjunto de acciones $\textit{A}$
\item Reglas de transición entre los estados
\item Reglas que determinen la recompensa para cada transición posible
\item Reglas que determinen qué es lo que un agente observa en un momento dado
\end{itemize}

El aprendizaje por refuerzo consta de dos etapas diferentes:

\begin{itemize}
\item Exploración: Durante esta etapa se toman decisiones con carácter aleatorio que permite expandir el espacio de búsqueda del agente.
\item Explotación: Donde el agente prioriza la maximización de la recompensa. Durante esta etapa el agente tomará decisiones que le reporten la mejor utilidad.
\end{itemize}

\section{Q-Learning}

El método elegido para el aprendizaje por refuerzo es Q-Learning. Q-Learning es una técnica de aprendizaje por refuerzos que se ajusta perfectamente a los requerimientos del simulador ya que no requiere que se modele el sistema completo para que este sea efectivo, en todo momento el agente solo está consciente de lo que le rodea y nada más.
El proceso de decisión del agente esta dado por la siguiente función de actualización:

\begin{equation}
Q(s_{t},a_{t}) \leftarrow Q(s_{t}, a_{t}) + 
	\alpha_{t}(s_{t},a_{t}) 
	\times \left [ 
		R(s_{t}) + 
		\gamma \; max_{\alpha_{t+1}}Q(s_{t+1},a_{t+1}) -
		Q(s_{t},a_{t})
	\right ]
\end{equation}

El valor que se va a actualizar de la tabla $Q(s_{t},s_{t})$ es la suma del valor viejo con una correción basada en la nueva informacion. El término 
$\alpha(s_{t},a_{t})$ es la taza de aprendizaje. Representa hasta que punto la nueva información reemplaza la anterior. Un valor de $0$ significa que el agente no ha aprendido nada, mientras que un valor de $1$ asegura que el agente considera solo la información más reciente.

La recompensa es representada por $R_{t+1}$ en base a la acción tomada $a_{t}$ para el estado $s_{t}$. El factor de descuento $\gamma$, determina la importancia de recompensas futuras. Un factor $0$ significa un agente oportunista que solo considera la recompensa actual, mientras que un valor que se acerque a $1$ hace que el agente se oriente hacia recompensas de largo plazo. Un valor mayor a cero hace que los valores de $Q$ diverga. El factor de descuento se aplica a la mejor acción que se puede tomar. El valor máximo de utilidad de la próxima acción esta dado por:
\begin{equation}
max_{\alpha_{t+1}}Q(s_{t+1},a_{t+1})
\end{equation}


\section{Trabajo Realizado}

\subsection{Simulador}

El simulador desarrollado cuenta con las siguientes características:
\begin{itemize}
\item Configuración de agentes por medio de archivo.
\item Configuración de entorno por medio de archivo.
\item Configuración de frame skip desde GUI. El usuario puede cambiar el frame skip del simulador. Un frameskip alto permite que la actualización visual se realice en una mayor de pasos. Esto permite liberar el procesador para realizar iteraciones de simulación mas rápidamente.
\item Permite guardar simulaciones en formato binario.
\item Permite reproducir simulaciones guardadas en formato binario.
\item Durante la simulación el usuario puede hacer click derecho sobre un agente para ver un panel que se actualiza en tiempo real con la posición y la descomposición de las fuerzas que actúan sobre él.
\item Durante la simulación el usuario puede hacer click para agregar un overlay sobre los agentes que indica la dirección y magnitud de las fuerzas que actúan sobre él.
\item Posibilidad de pausar una simulación.
\item Posibilidad de reiniciar una simulación.
\item El simulador guarda el estado de los agentes que utilizan Q-Learning a archivos de log para su posterior análisis.
\end{itemize}

Se presta soporte para la declaración de agentes de variados diámetros y masas, permitiendo también al usuario que declare que tipo de movimiento deberá seguir el agente. También se define cual será el punto hacia el cual el agente deberá ir.
Se pueden declarar también obstáculos de formas geométricas arbitrarias.

Finalmente se permite la declaración de sumideros (puntos de llegada o metas) y de generadores (puntos de donde se generan agentes).

El simulador fue desarrollado utilizando \textit{Java} y \textit{Swing}. Consta de una implementación de interfaz que utiliza extensivamente \textit{Worker Threads} para garantizar la capacidad de respuesta del sistema al mismo tiempo que previene problemas de concurrencia.

La simulación de agentes se realiza utilizando \textit{Thread Pools} que permiten paralelizar los cálculos dentro de una misma iteración y mejorar el tiempo que toman las mismas.

Teniendo en cuenta ciertas dificultades que se encontraron durante el desarrollo, principalmente en el área de GUI, 
consideramos que \textit{Java}, si bien es un lenguaje muy flexible, puede no haber sido la mejor elección sobre la cual 
construir el proyecto.
Una mejor elección podría ser la utilización de un motor de video juegos para poder aprovechar la sincronización de 
threads y las vastas librerías de vectores y física que estos motores poseen. Entre ellos los que más llaman la atención 
son:

\begin{itemize}
\item Cocos2d: (http://cocos2d.org/) es una librería de Python opensource que existe desde febrero de 2008. Su principal 
ventaja sobre la utilización de JAVA es como ya se mencionó, que este motor permite un manejo mas cómodo de la 
graficación. Su otra ventaja es que al estar basada sobre OpenGL provee una interfaz mas fluida al usuario.
\item Unity3D: (http://unity3d.com/) es un motor de vídeo juegos con soporte nativo multiplataforma. Si bien no es 
opensource como Cocos2d, provee con un entorno de edición que facilita tareas típicas que acelera mucho el 
desarrollo. En nuestra opinión esta es la mejor de las 2 opciones.
\end{itemize}

\subsection{Modos de Uso}

El simulador cuenta con dos modos principales:

\begin{itemize}
\item Simulación: Donde el uusario configura el simulador con archivos de entorno y agentes y realiza las simulaciones.
\item Reproducción o Replay: Donde el usuario configura el simulador con un archivo previamente generado que guarda el estado de los agentes en cada intervalo de tiempo.
\end{itemize}

\begin{figure}
\centering
\includegraphics{images/simuladorplay.png}
\caption[Sim Play]{\small{Simulador en Modo Play}}
\label{figSimPlay}
\end{figure}

\begin{figure}
\centering
\includegraphics{images/simuladorreplay.png}
\caption[Sim Replay]{\small{Simulador en Modo Replay}}
\label{figSimReplay}
\end{figure}

Cuando el simulador se encuentra en modo reproducción el usuario tiene la posibilidad de avanzar cuadro por cuadro, 
reproducir normalmente o ir al principio o final. Así mismo el simulador provee un deslizador para que el usuario pueda situar 
la simulación en cualquier punto.

\subsection{Fuerza Social}
El modelo de \textit{Fuerza Social} \cite{helbing:panic} se implementó sin modificaciones. El simulador soporta agentes de variados diámetros y masas.
Los parámetros de los agentes se especifícan en los archivos
de simulación.
Los parámetros utilizados en los cálculos de la Fuerza Social son los siguientes:
\begin{itemize}
\item Diferencial de tiempo: $dt \; = \; 0.01s$.
\item Velocidad de Deseo: $v_{i}^{0} \; = \; 1,4m/s$. 

Pues esa es la velocidad típica a la que caminan las personas.

\end{itemize}
Es posible determinar los parámetros de Q-Learning en el menú del Simulador. 
En nuestras simulaciones se usaron los siguientes parametros:
\begin{itemize}
\item Factor de Aprendizaje ($\alpha$) = $0.1$
\item Factor de Descuento ($\gamma$) = $0.9$
\end{itemize}

\subsection{Implementación de Q-Learning}

\subsubsection{Modelización de los estados $S$}

Los estados han sido modelados como un vector de estado de sensores dentro del agente. Este vector representa el campo visual del agente. Se decidó aproximar el campo visual del ser humano a 180 grados hacia el frente.

Los sensores se sitúan cubriendo todo el espectro visual, con mayor concentración en la zona frontal, (ver figura de distribución de sensores). Inicialmente se evaluó la posibilidad de utilizar 6 sensores distribuidos uniformemente a lo largo del arco visual del agente pero luego de realizar experimentos con esta configuración se optó por un modelo de 8 sensores con mayor concentración en la zona frontal.

Se probaron ambas alternativas en diversos escenarios y observamos mejoras de hasta 40\% en llegadas exitosas a destino versus colisiones. El costo viene con el cáluclo del sensado, que toma un 25\% más.

Los agentes tienen capacidad de observar hasta una distancia de 10m.

La forma en la que el agente releva los datos se refleja en el estado con una escala de 3 valores: 
\begin{itemize}
\item 0 significa que no se observa nada en ese sensor.
\item 1 y 2 significan que el agente detecta riesgo en ese sensor. el valor del sensor depende directamente de la velocidad relativa observada por el agente en sentido normal para un sensor dado.
\end{itemize}

En total tenemos 6561 estados posibles.

La fórmula que calcula el riesgo entre el agente $i$ y el agente $k$ es la siguiente:

\begin{equation}
R_{ik} = ((\vec{v}_{i} - \vec{v}_{k}) \cdot (\vec{p}_{i} - \vec{p}_{k}))
\end{equation}

Donde $\vec{v}$ es la velocidad y $\vec{p}$ es la posición de los respectivos agentes. 

El riesgo se interpreta de la siguiente manera:

\begin{itemize}

\item $0$ si $R_{ik} \le 0$
\item $1$ si $0 < R_{ik} \le v_{i}^{0}/2$
\item $2$ si $R_{ik} > v_{i}^{0}/2$
\end{itemize}

Donde $v_{i}^{0}$ es la velocidad de deseo del agente.

En la figura \ref{figSensores} tenemos un esquema del rango de visión de los sensores en las distintas alineaciones. La configuración de 8 sensores aprovecha huecos en el arco frontal de visión que la configuración de 6 sensores es incapaz de reconocer.
\begin{figure}
\centering
\includegraphics{images/sensores.png}
\caption[Sensores]{\small{Configuración de 6 sensores contra 8 sensores.}}
\label{figSensores}
\end{figure}

\subsection{Modelización de las Acciones $A$}

Las acciones que el agente puede tomar son las siguientes:
\begin{center}
\begin{tabular}{ | c | p{10cm} |}
\hline
\textbf{Nombre} & \textbf{Descripción} \\ \hline \hline
ACTION\_NONE & El agente no hace nada para cambiar su trayectoria. El agente elige esta acción cuando no detecta amenazas. \\ \hline
ACTION\_BACK  & El agente activa un actuador que le imprime una fuerza de $0.5 \times v_{i}^{0}$ en dirección opuesta a su velocidad actual. \\ \hline
ACTION\_LEFT & El agente activa una actuador que le imprime una fuerza de $v_{i}^{0}$ hacia su izquierda. \\ \hline
ACTION\_RIGHT & El agente activa un actuador que le imprime una fuerza de $v_{i}^{0}$ hacia su derecha. \\ \hline
\end{tabular}
\end{center}

Donde $v_{i}^{0}$ es la velocidad de deseo del agente.

\subsection{Esquema de Refuerzos}
Se decidió por un esquema de refuerzos negativos. Cada vez que el agente choca con otro agente se refuerza con un valor de $-1$. La decisión se basa en que al seguir presente la fuerza de deseo mientras el agente aprende utilizando Q-Learning no es necesario recompensarlo por avanzar correctamente.

\subsection{Exploración vs Explotación}

Inicialmente el agente tiene una probabilidad de exporar del 100\%. Esta probabilidad decrece a lo largo de la simulación, forzando a que se explote más mientras mas avanzada este la simulación. Toda exploración termina antes del 66\% de la simulación. Desde ahí en adelante el agente solo explota las decisiones aprendidas, no explora nuevas. En la figura \ref{figexploracionvsexplotacion} se observa como decrece la probabilidad de exploración versus los pasos de simulación transcurridos.

\begin{figure}
\centering
\includegraphics{images/exploracionvsexplotacion.png}
\caption[figexploracionvsexplotacion]{\small{Probabilidad de exploración vs iteraciones de Q-Learning}}
\label{figexploracionvsexplotacion}
\end{figure}

\section{Experimentos y Resultados}

A continuación se presentan los resultados de los 4 experimentos propuestos. Los supuestos para las simulaciones son los siguientes:

\begin{itemize}
\item Se entrenó el agente drante $1000$ pasos de decisión de Q-Learning.
\item Se evalúa Q-Learning cada el equivalente a $200ms$ en tiempo de simulación.
\item El \textit{decay rate} (el factor que decide cuando un agente debe explorar y cuando explotar) vale $0.15$. Esto quiere decir que en $2/3$ de la simulación el agente deja de explorar por completo.
\item Solo se otorgan refuerzos negativos cuando el agente colisiona con otro agente o con un obstáculo.
\item No se otorgan refuerzos positivos.
\item El desempeño de una simulación se mide por el ratio \textit{llegadas}/\textit{colisiones}.
\item Cuando debe decidir, el agente elige la acción con utildad menos negativa.
\end{itemize}

\subsection{Formatos de Salida}

Las simulaciones arrojan resultados en formato de log, los cuales se analizan para construir gráficos para dos métricas distintas:

\begin{itemize}
\item Desempeño: Representa la cantidad de llegadas con éxito contra la cantidad de colisiones que sufrió un agente.
\item Velocidad: Representa la velocidad a la que se trasladó el agente durante el intervalo de tiempo entre decisiones de Q-Learning.
\end{itemize}

Se loguea la matriz-Q que resultó del experimento para poder utilizarla en futuras simulaciones. El formato de la matriz crudo es algo difícil de analizar por lo que los resultados que aquí se presentan se acompaña las matrices en un formato mas ordenado.

El estado de la matriz-Q debe interpretarse de izquierda a derecha, es decir que el primer valor del vector corresponde al sensor de la derecha y el último al último sensor de la izquierda.

Los archivos de log tienen el siguiente formato:

\begin{figure}
\centering
\includegraphics{images/formatosim.png}
\caption[Sensores]{\small{Formato de archivo \textit{log} analizado.}}
\label{figFormatoSim}
\end{figure}

\subsection{Experimento 1}

Dos agentes alineados que se enfrentan y deben eludirse con la política aprendida.

Para aprender se usarán 3 esquemas independientes, la política hallada en cada uno debe ser usada para llevar a cabo el experimento final 1 y comparar cuál esquema de aprendizaje funciona mejor.

\begin{itemize}

\item Experimento 1.1 - Un sistema de un agente con un obstáculo (un agente quieto) en la línea del target. La posición inicial del agente debe ser  aleatoria alrededor de la línea que pasa por el obstáculo, de modo que lo eluda a  veces por derecha y a veces por izquierda.

\item Experimento 1.2 - Ídem pero la velocidad del agente es duplicada.

\item Experimento 1.3 - El obstáculo ahora es móvil. Es un agente que se mueve con el modelo de fuerza social reducida (solamente tiene fuerza de deseo y de contacto). El target del obstáculo queda directamente detrás del agente que aprende con \textit{Q-Learning}.

\end{itemize}

\begin{figure}
\centering
\includegraphics{images/ej1.png}
\caption[Exp 1]{\small{Experimento 1 - Dos agentes encontrados.}}
\label{figExp1}
\end{figure}

El Experimento 1.1 es una prueba simple de la capacidad del simulador. En los 2 primeros experimentos propuestos el agente que aprende se comporta como se esperaba y tiene una performance positiva. Las figuras \ref{figExpRes1}, \ref{figExpRes2}, \ref{figExpRes3} son los gráficos de desempeño de los experimentos. La línea azul representa las llegadas con éxito, y la roja las colisiones.

\begin{figure}
\centering
\includegraphics{images/ej1_1res.png}
\caption[Exp 1 Res]{\small{Experimento 1.1 - Performance}}
\label{figExpRes1}
\end{figure}

\begin{figure}
\centering
\includegraphics{images/ej1_2res.png}
\caption[Exp 2 Res]{\small{Experimento 1.2 - Performance}}
\label{figExpRes2}
\end{figure}

\begin{figure}
\centering
\includegraphics{images/ej1_3res.png}
\caption[Exp 3 Res]{\small{Experimento 1.3 - Performance}}
\label{figExpRes3}
\end{figure}

El análisis de los gráficos de desempeño revela importantes consecuencias del comportamiento del agente que aprende.
\begin{itemize}

\item En el Experimento 1.1 la performance experimenta una mejoría notable a partir de la mitad de la simulación. Esto es causado por la finalización de la etapa de exploración, deviniendo en una menor posibilidad de movimientos aleatorios y el aprovechamiento de la estrategia aprendida (figura \ref{figExpRes1}).

\item En el Experimento 1.2 (figura \ref{figExpRes2}) se nota claramente el punto de inflexión entre exploración y explotación. La diferencia con el Experimento 1.1 es la velocidad del agente, que es el doble. Esto efectivamente duplica la dificultad del agente para en llegar con éxito a destino.

\end{itemize}

En las \textit{matrices de Q} se puede ver que los valores de utilidad son los esperados en casi todos los casos. Recordar
que el primer sensor es el que esta más a la derecha del agente, es decir, el que tiene un ángulo 0 con el campo de visión. Las siguientes posiciones del vector representan los vectores a medida que el angulo incrementa entre el sensor y el campo de visión. Otro elemento a tener en cuenta es que los sensores no se distribuyen equivalentemente, si no mas bien
con una mayor concentración en los 4 sensores centrales.

\begin{table}
\begin{center}
\begin{tabular}{ | p{3cm} | c | c | c | c |}

\hline
Sensado                     &  LEFT & RIGHT & NONE  & BACK \\ \hline
0,0, 0,0, 1,0, 0,0 & 0.000000  &  \textbf{1.836658}   &  0.000000   & 0.000000  \\ \hline
0,0, 0,0, 0,0, 0,0 & 1.338391   & 1.263650   &  \textbf{1.616758}   & 1.296270  \\ \hline
0,1, 0,0, 0,0, 0,0 & -0.190000  &  2.044788  &   0.976724  &  \textbf{3.995214} \\ \hline
1,0, 0,0, 0,0, 0,0 & 0.000000   & 2.606554   &  3.533919   & \textbf{5.292406} \\ \hline
0,0, 0,1, 1,0, 0,0 & 0.107069   & \textbf{5.984513}   &  0.000000   & 0.000000 \\ \hline
0,0, 1,0, 0,0, 0,0 & -0.100000  &  0.384343  &   0.302075  &  \textbf{2.437552} \\ \hline
0,0, 0,1, 0,0, 0,0 & 0.782559   & \textbf{2.040882}  &   0.947097   & 1.897630 \\ \hline

\end{tabular}
\caption{\small{Matriz de Q - Experimento 1.1}}
\label{QMatrixExp1.1}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{ | p{3cm} | c | c | c | c |}
\hline
Sensado                     &  LEFT & RIGHT & NONE  & BACK \\ \hline
0,0, 1,0, 0,0, 0,0 & -0.271000  &  -0.091080   &  -0.294573  &  \textbf{1.124596} \\ \hline
0,0, 0,0, 0,0, 1,0 & \textbf{6.092650}   & 0.437323    & 0.458220   & 0.000000 \\ \hline
0,0, 0,2, 2,0, 0,0 & 0.674703   & 1.052398    & 0.990000   & \textbf{8.502963} \\ \hline
0,0, 0,0, 0,0, 0,1 & 0.000000   & 0.000000    & \textbf{6.590362}   & 1.190996 \\ \hline
0,0, 0,0, 1,0, 0,0 & \textbf{6.790741}   & 1.412947    & 2.451154   & 1.753274 \\ \hline
0,1, 0,0, 0,0, 0,0 & 0.000000   & 1.458494    & 1.824140   & \textbf{3.999286} \\ \hline
0,0, 0,1, 0,0, 0,0 & \textbf{2.799878}   & 0.278172    & 0.562676   & 0.514212 \\ \hline
0,0, 0,1, 1,0, 0,0 & \textbf{7.952153}   & 1.125422    & 0.877281   & 0.000000 \\ \hline
1,0, 0,0, 0,0, 0,0 & 0.000000   & -0.100000    & \textbf{3.409122}   & 0.000000 \\ \hline
0,0, 0,2, 0,0, 0,0 & 0.527716   & \textbf{5.445163}    & 5.234501   & 5.248148 \\ \hline
0,0, 0,0, 2,0, 0,0 & 2.458329   & 4.871357    & 3.841234   & \textbf{5.929559} \\ \hline
0,0, 0,0, 0,0, 0,0 & 2.983337   & 3.061287    & \textbf{3.950212}   & 2.933851 \\ \hline
0,0, 0,0, 0,1, 0,0 & \textbf{5.547692}   & 0.000000    & -0.100000   & 0.455750 \\ \hline

\end{tabular}
\caption{\small{Matriz de Q - Experimento 1.2}}
\label{QMatrixExp1.2}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{ | p{3cm} | c | c | c | c |}
\hline
Sensado                     &  LEFT & RIGHT & NONE  & BACK \\ \hline
0,0, 1,0, 0,0, 0,0 &  0.000000  & 2.607660  &   2.537028  &  \textbf{4.343992} \\ \hline
0,0, 0,0, 1,0, 0,0 & 3.092437   & \textbf{3.271224}  &   3.127772  &  3.015895 \\ \hline
0,0, 0,1, 1,0, 0,0 & 0.616420   & \textbf{6.633904}  &   1.095116  &  1.609967 \\ \hline
0,1, 0,0, 0,0, 0,0 & 0.000000   & 4.580436  &   3.459806  & \textbf{4.894548} \\ \hline
1,0, 0,0, 0,0, 0,0 & 0.000000   & 4.607929  &   \textbf{5.665124}  &  4.922007 \\ \hline
0,0, 0,0, 0,0, 0,0 & 2.838395   & \textbf{3.380589}  &   3.100250  &  2.807318 \\ \hline
0,0, 0,2, 2,0, 0,0 & 1.497292   & 0.000000  &   \textbf{4.266430}  &  1.418284 \\ \hline
0,0, 0,1, 0,0, 0,0 & 2.827251   & 3.542712  &   2.995507  &  \textbf{4.254867} \\ \hline
0,0, 0,2, 0,0, 0,0 & 1.272467   & 6.238893  &   5.538935  &  \textbf{6.844985} \\ \hline
0,0, 0,0, 2,0, 0,0 & 0.000000   & \textbf{4.481721}  &   3.095885  &  2.232653 \\ \hline

\end{tabular}
\caption{\small{Matriz de Q - Experimento 1.3}}
\label{QMatrixExp1.3}
\end{center}
\end{table}

\section{Conclusión}

\bibliographystyle{abbrv}
\bibliography{01}


\end{document}
