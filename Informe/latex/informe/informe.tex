\documentclass[12pt]{article}

\usepackage[latin1]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[a4paper,left=3cm,right=3cm]{geometry}
\usepackage{graphicx}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\usepackage{longtable}

\title{Proyecto Final: Simulador de Muchedumbres}
\date{Junio, 2012}


\begin{document}

\begin{titlepage}
\begin{center}

\includegraphics[width=0.36\textwidth]{images/logo_itba.png}\\[1cm]

\textsc{\LARGE Instituto Tecnológico de Buenos Aires}\\[1.5cm]
\textsc{\Large Proyecto Final de Grado}\\[0.5cm]

\HRule \\[0.4cm]
{ \huge \bfseries Simulador de Muchedumbres}\\[0.4cm]

\HRule \\[1.5cm]


\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Autores:}\\
David \textsc{Grandes}\\dgrandes@alu.itba.edu.ar
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\vspace{6 mm}
Matías \textsc{Pan}\\mpan@alu.itba.edu.ar
\end{flushright}
\end{minipage}
\\[1.5cm]
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Tutores:}\\
Juan \textsc{Santos}\\jsantos@itba.edu.ar
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\vspace{6 mm}
Daniel \textsc{Parisi}\\dparisi@itba.edu.ar
\end{flushright}
\end{minipage}

\vfill

{\large Julio 2012}

\end{center}
\end{titlepage}

\pagebreak

\section{Resumen}

Este proyecto tiene como objetivo la realización de un Simulador de Muchedumbres que permita la investigación de distintas políticas de movimiento. Se investigará el desempeño de técnicas de \textit{aprendizaje por refuerzo} en la navegación de los agentes en distintos entornos.

La modelización y simulación de muchedumbres es un problema de interés para varias disciplinas de ingeniería y logística. El objetivo la investigación de \textit{aprendizaje por refuerzo} como política en la simulación de muchedumbres, con el deseo de encontrar una manera más realista de simular la navegación/elusión de obstáculos fijos o móviles (otros peatones).

La investigación de la efectividad de políticas de navegación se realizó a través de un simulador. El simulador fue implementado por el equipo y permitió la creación de entornos y distintos tipo de agentes para evaluar el aprendizaje. A su vez el simulador graba las simulaciones y las reproduce posteriormente para poder analizar los resultados.

Los resultados a favor del \textit{aprendizaje por refuerzo} son variados. Se observan mejoras en ciertos entornos simples, pero en otros mas complejos, donde el agente debe interactuar con alta densidad de obstáculos se nota una tendencia a sobrecompensar y fallar repetidamente al intentar eludirlos.

\pagebreak

\tableofcontents

\pagebreak

\section{Introducción}

En la actualidad existen diversos métodos que simulan el comportamiento de agentes independientes moviéndose en un entorno. Uno de los mecanismos más sencillos es el uso de fuerzas, atractivas o repulsivas, que gobiernen el movimiento de los agentes. Este mecanismo tiene serias deficiencias en entornos complejos, o con caminos no directos entre el agente y su destino. Si el punto de deseo del agente se encuentra detrás de una pared la fuerza normal de la misma lo repele, dejando al agente atrapado en el lugar donde la fuerza resultante es 0.

Lo que se propone es modificar este modelo por uno de aprendizaje reforzado. El aprendizaje por refuerzo es un área de estudio que contempla como un agente toma decisiones en un entorno no totalmente conocido. El agente maximiza la utilidad mediante refuerzos, dependiendo si tomó una buena o mala decisión. A su vez el agente tiene que encontrar un balance apropiado entre la exploración de mejores resultados y la explotación de conocimiento ya establecido.

Se abordará el problema utilizando el método de aprendizaje por refuerzo llamado Q-Learning. Este método consiste en aprender asignando diferentes utilidades. El agente cambia de un estado a otro realizando una acción a $\in$ A. La función que define la calidad, la cual el agente intentará maximizar a la hora de tomar decisiones, es:

\begin{equation}
\label{eq-qlearning1}
Q = S \times A \rightarrow R
\end{equation}

Se entrenará utilizando aprendizaje por refuerzo en 3 entornos diferentes y luego se analizará el comportamiento en un entorno donde no podrá aprender sino que deberá explotar el conocimiento ya adquirido. Los agentes que no utilicen Q-Learning se regirán por el modelo de fuerza social.\cite{helbing:panic}

\section{Antecedentes}
\subsection{Modelo de Fuerza Social}

El modelo de fuerza social \cite{helbing:panic} consiste en modelar el comportamiento como una combinación de fuerzas físicas que intentan representar comportamientos:

\begin{itemize}
\item Fuerza de Deseo 
\item Fuerza Social   
\item Fuerza Granular o de Contacto
\end{itemize}

La fuerza de deseo es el comportamiento natural del agente, que es de ir a donde el desea. La fuerza social representa la tendencia que presentan las personas a permaner alejadas de las demás. La fuerza granular o de contacto son fuerzas que se manifiestan durante el contacto entre individuos u obstáculos.

La ecuación general de movimiento del agente es la siguiente:
\begin{equation}
m_{i}\dfrac{d \mathbf{v}_{i}}{dt} = m_{i} \frac{
	v^{0}_{i}(t) \mathbf{e}_{i}^{0}(t) - \mathbf{v}_{i}(t)
}
{
\mathbf{\tau}_{i}
}
+ \sum_{j(\ne i)}\mathbf{f}_{ij} 
+ \sum_{W}\mathbf{f}_{iW}
\end{equation}

donde $\mathbf{f}_{ij}$ son las fuerzas entre los agentes y $\mathbf{f}_{iW}$ son las fuerzas entre el agente y los obstáculos.

\subsubsection{Fuerza de Deseo}
El primer término de la ecuación es la \textit{fuerza de deseo} del agente, donde $\mathbf{e}_{i}^{0}$ es el vector de dirección al punto de interés. $v_{i}^{0}$ es la velocidad deseada, la cual es un factor en la velocidad instantanea. $\tau_{i}$ representa el tiempo de reacción o demora que tiene el agente en cambiar su velocidad.

\subsubsection{Fuerza Social y Granular}
Las fuerzas entre agentes y obstáculos tienen dos componentes, la \textit{fuerza social} y la \textit{fuerza granular}.
Definimos las fuerzas entre los agentes ($\mathbf{f}_{ij}$) de la siguiente manera:
\begin{equation}
\mathbf{f}_{ij} = \left \{ A_{i}\; \exp \left [ \dfrac{r_{ij} - d_{ij}}{B_{i}}\right ] 
+ 
\kappa g(r_{ij} - d_{ij})
\right \}\mathbf{n}_{ij}
+
\kappa g(r_{ij} - d_{ij}) \Delta v_{ij}^{t}\mathbf{t}_{ij}
\end{equation}


La \textit{fuerza social} esta incluida en el primer término de la ecuación:
\begin{equation}
A_{i}\; \exp \left [ \dfrac{r_{ij} - d_{ij}}{B_{i}}\right ] 
\end{equation}

Es una fuerza de repulsión entre ellos que se hace más grande a medida que se acercan. $A_{i}$ y $B_{i}$ son constantes. $d_{ij}\; =\; ||r_{i}-r_{j}||$ y representa la distancia entre los centros de masa de los agentes. El versor $\mathbf{n}_{ij}$ apunta del agente j al agente i, lo cuál hace a esta una fuerza de repulsión. 

La \textit{fuerza granular} o la \textit{fuerza de contacto} es una fuerza que se manifiesta solamente cuando el agente entra en contacto con otros elementos. Esta fuerza tiene dos componentes. La primera es una fuerza de resistencia de un cuerpo al ser comprimido por una fuerza externa. Esta dada por:
\begin{equation}
kg(r_{ij} - d_{ij})\mathbf{n}_{ij}
\end{equation} 
Esta fuerza va en la misma dirección normal que la \textit{fuerza social}. $\kappa$ es una constante. $g()$ es una función escalonada, que es $1$ si los agentes estan en contacto y $0$ si no. Es esta función la que habilita las fuerzas granulares en la ecuación de movimiento. 

La segunda componente es una fuerza que impide el movimiento tangencial relativo. Esta dada por:
\begin{equation}
\kappa g(r_{ij} - d_{ij}) \Delta v_{ij}^{t}\mathbf{t}_{ij}
\end{equation}

$\mathbf{t}_{ij}$ es la dirección tangencial y esta dada por $\mathbf{t}_{ij}\; =\; (-n_{ij},n_{ij})$. $\Delta v_{ij}^{t}$ es la diferencia de velocidad tangencial entre los agentes.

La fuerza entre el agente y los obstaculos, en nuestro caso las paredes, esta dada por:
\begin{equation}
\mathbf{f}_{iW} = \left \{ A_{i}\; \exp \left [ \dfrac{r_{i} - d_{iW}}{B_{i}}\right ] 
+ 
\kappa g(r_{i} - d_{iW})
\right \}\mathbf{n}_{iW}
+
\kappa g(r_{i} - d_{iW})(\mathbf{v}_{i} \cdot \mathbf{t}_{iW})\mathbf{t}_{iW}
\end{equation}

La ecuación es analoga a la de los agentes salvo por la dirección, que en este caso es normal a la superficie de la pared. Podemos ver que la fuerza social esta presente en esta ecuación también. 

\subsubsection{Desventajas}

La principal desventaja del modelo de fuerza social, y el motivo por el cúal se busca una alternativa a este, es que los agentes tienden a atascarse cuando se enfrentan a un obstáculo que yace en la dirección al objetivo \textit{figura \ref{figProbFuerzaSocial}} . Se pretende solucionar este problema permitiendo al agente ``eludir'' obstáculos antes de llegar a la situación de contacto.

\begin{figure}
\centering
\includegraphics{images/fuerzasocial.png}
\caption[Fuerza Social]{\small{Situaciones problemáticas para el modelo de fuerza social}}
\label{figProbFuerzaSocial}
\end{figure}

\subsection{Aprendizaje por Refuerzo}
El aprendizaje por refuerzo está fuertemente inspirado por la psicología conductista y el concepto de enseñar por medio de recompensas las decisiones que toma el individuo.

El model consiste de los siguientes elementos:
\begin{itemize}
\item conjunto de estados $\textit{S}$
\item conjunto de acciones $\textit{A}$
\item reglas de transición entre los estados
\item reglas que determinen la recompensa para cada transición posible
\item reglas que determinen que es lo que un agente observa en un momento dado
\end{itemize}

El aprendizaje por refuerzo consta de dos etapas diferentes:

\begin{itemize}
\item Exploración: Durante esta etapa se toman decisiones con carácter aleatorio que permite expandir el espacio de búsqueda del agente.
\item Explotación: Donde el agente prioriza la maximización de la recompensa. Durante esta etapa el agente tomará decisiones que le reporten la mejor utilidad.
\end{itemize}

\section{Q-Learning}

El método elegido para el aprendizaje por refuerzo es Q-Learning. Q-Learning es una técnica de aprendizaje por refuerzos que se ajusta perfectamente a los requerimientos del simulador ya que no requiere que se modele el sistema completo para que este sea efectivo, en todo momento el agente solo está conciente de lo que le rodea y nada más.
El proceso de decisión del agente esta dado por la siguiente función de actualización:

\begin{equation}
Q(s_{t},a_{t}) \leftarrow Q(s_{t}, a_{t}) + 
	\alpha_{t}(s_{t},a_{t}) 
	\times \left [ 
		R(s_{t}) + 
		\gamma \; max_{\alpha_{t+1}}Q(s_{t+1},a_{t+1}) -
		Q(s_{t},a_{t})
	\right ]
\end{equation}

El valor que se va a actualizar de la tabla $Q(s_{t},s_{t})$ es la suma del valor viejo con una corrección basada en la nueva informacion. El término 
$\alpha(s_{t},a_{t})$ es la tasa de aprendizaje y representa hasta que punto la nueva información reemplaza la anterior. Un valor de $0$ significa que el agente no ha aprendido nada, mientras que un valor de $1$ asegura que el agente considera solo la información más reciente.

La recompensa es representada por $R_{t+1}$ en base a la acción tomada $a_{t}$ para el estado $s_{t}$. El factor de descuento $\gamma$, determina la importancia de recompensas futuras. Un factor $0$ significa un agente oportunista que solo considera la recompensa actual, mientras que un valor que se acerque a $1$ hace que el agente se oriente hacia recompensas de largo plazo. Un valor mayor a $1$ hace que los valores de $Q$ diverga. El factor de descuento se aplica a la mejor acción que se puede tomar. El valor máximo de utilidad de la próxima acción esta dado por:
\begin{equation}
max_{\alpha_{t+1}}Q(s_{t+1},a_{t+1})
\end{equation}


\section{Trabajo Realizado}

\subsection{Simulador}

El simulador desarrollado cuenta con las siguientes características:
\begin{itemize}
\item Configuración de agentes por medio de archivo.
\item Configuración de entorno por medio de archivo.
\item Configuración de salto de cuadros desde GUI. El usuario puede cambiar el salto de cuadros del simulador. Un valor de salto de cuadros alto permite que la actualización visual se realice en una cantidad mayor de pasos. Esto permite liberar el procesador para realizar iteraciones de simulación mas rápidamente.
\item Permite guardar simulaciones en formato binario.
\item Permite guardar simulaciones de manera comprimida.
\item Permite reproducir simulaciones guardadas en formato binario y/o comprimidas.
\item Durante la simulación el usuario puede hacer click derecho sobre un agente para ver un panel que se actualiza en tiempo real con la posición y la descomposición de las fuerzas que actúan sobre él.
\item Durante la simulación el usuario puede hacer click para agregar un indicador sobre los agentes que indica la dirección y magnitud de las fuerzas que actúan sobre él.
\item Posibilidad de pausar una simulación.
\item Posibilidad de reiniciar una simulación.
\item El simulador guarda el estado de los agentes que utilizan Q-Learning en archivos de log para su posterior análisis.
\end{itemize}

Se presta soporte para la declaración de agentes de variados diámetros y masas, permitiendo también al usuario que declare el tipo de movimiento deberá seguir el agente. También se define cuál será el punto hacia el cual el agente deberá ir.
Se pueden declarar también obstáculos de formas geométricas arbitrarias.

Finalmente se permite la declaración de sumideros (puntos de llegada o metas) y de generadores (puntos de donde se generan agentes).

El simulador fue desarrollado utilizando \textit{Java} y \textit{Swing}. Consta de una implementación de interfaz que utiliza extensivamente \textit{Worker Threads} para garantizar la capacidad de respuesta del sistema al mismo tiempo que previene problemas de concurrencia.

La simulación de agentes se realiza utilizando \textit{Thread Pools} que permiten paralelizar los cálculos dentro de una misma iteración y mejorar el tiempo que toman las mismas.

\subsection{Modos de Uso}

El simulador cuenta con dos modos principales:

\begin{itemize}
\item Simulación: Donde el usuario configura el simulador con archivos de entorno y agentes y realiza las simulaciones.
\item Reproducción o Replay: Donde el usuario configura el simulador con un archivo previamente generado que guarda el estado de los agentes en cada intervalo de tiempo. Ver \textit{figura \ref{figSimPlay}} y \textit{figura \ref{figSimReplay}}.
\end{itemize}

\begin{figure}
\centering
\includegraphics{images/simuladorplay.png}
\caption[Sim Play]{\small{Simulador en Modo Play}}
\label{figSimPlay}
\end{figure}

\begin{figure}
\centering
\includegraphics{images/simuladorreplay.png}
\caption[Sim Replay]{\small{Simulador en Modo Replay}}
\label{figSimReplay}
\end{figure}

Cuando el simulador se encuentra en modo reproducción el usuario tiene la posibilidad de avanzar cuadro por cuadro, 
reproducir normalmente o ir al principio o final. Así mismo el simulador provee un deslizador para que el usuario pueda situar 
la simulación en cualquier punto.

\subsection{Fuerza Social}
El modelo de \textit{Fuerza Social} \cite{helbing:panic} se implementó sin modificaciones. El simulador soporta agentes de variados diámetros y masas.
Los parámetros de los agentes se especifícan en los archivos
de simulación.
Los parámetros utilizados en los cálculos de la Fuerza Social son los siguientes:
\begin{itemize}
\item Diferencial de tiempo: $dt \; = \; 0.01s$.
\item Velocidad de Deseo: $v_{i}^{0} \; = \; 1,2m/s$. (Es la velocidad típica a la que caminan las personas)

\end{itemize}
Es posible determinar los parámetros de Q-Learning en el menú del Simulador. 
En nuestras simulaciones se usaron los siguientes parámetros:
\begin{itemize}
\item Factor de Aprendizaje ($\alpha$) = $0.1$
\item Factor de Descuento ($\gamma$) = $0.9$
\end{itemize}

\subsection{Implementación de Q-Learning}

\subsubsection{Modelización de los estados $S$}

Los estados han sido modelados como un vector de estado de sensores dentro del agente. Este vector representa el campo visual del agente. Se decidó aproximar el campo visual del ser humano a 180 grados hacia el frente.

Los sensores se sitúan cubriendo todo el espectro visual, con mayor concentración en la zona frontal, (ver figura de distribución de sensores). Inicialmente se evaluó la posibilidad de utilizar 6 sensores distribuidos uniformemente a lo largo del arco visual del agente pero luego de realizar experimentos con esta configuración se optó por un modelo de 8 sensores con mayor concentración en la zona frontal.

Se probaron ambas alternativas en diversos escenarios y observamos mejoras de hasta 40\% en llegadas exitosas a destino versus colisiones. El costo viene con el cálculo del sensado, que toma un 25\% más.

Los agentes tienen capacidad de observar hasta una distancia de 10m.

La forma en la que el agente releva los datos se refleja en el estado con una escala de 3 valores: 
\begin{itemize}
\item 0 significa que no se observa nada en ese sensor.
\item 1 y 2 significan que el agente detecta riesgo en ese sensor. el valor del sensor depende directamente de la velocidad relativa observada por el agente en sentido normal para un sensor dado.
\end{itemize}

En total tenemos 6561 estados posibles.

La fórmula que calcula el riesgo entre el agente $i$ y el agente $k$ es la siguiente:

\begin{equation}
R_{ik} = ((\vec{v}_{i} - \vec{v}_{k}) \cdot (\vec{p}_{i} - \vec{p}_{k}))
\end{equation}

donde $\vec{v}$ es la velocidad y $\vec{p}$ es la posición de los respectivos agentes. 

El riesgo se interpreta de la siguiente manera:

\begin{itemize}

\item $0$ si $R_{ik} \le 0$
\item $1$ si $0 < R_{ik} \le v_{i}^{0}/2$
\item $2$ si $R_{ik} > v_{i}^{0}/2$
\end{itemize}

donde $v_{i}^{0}$ es la velocidad de deseo del agente.

En la \textit{figura \ref{figSensores}} tenemos un esquema del rango de visión de los sensores en las distintas alineaciones. La configuración de 8 sensores aprovecha huecos en el arco frontal de visión que la configuración de 6 sensores es incapaz de reconocer.
\begin{figure}
\centering
\includegraphics{images/sensores.png}
\caption[Sensores]{\small{Configuración de 6 sensores contra 8 sensores.}}
\label{figSensores}
\end{figure}

\subsection{Modelización de las Acciones $A$}

Las acciones que el agente puede tomar son las siguientes:
\begin{center}
\begin{tabular}{ | c | p{10cm} |}
\hline
\textbf{Nombre} & \textbf{Descripción} \\ \hline \hline
ACTION\_NONE & El agente no hace nada para cambiar su trayectoria. El agente elige esta acción cuando no detecta amenazas. \\ \hline
ACTION\_BACK  & El agente activa un actuador que le imprime una fuerza de $0.5 \times v_{i}^{0}$ en dirección opuesta a su velocidad actual. \\ \hline
ACTION\_LEFT & El agente activa un actuador que le imprime una fuerza de $v_{i}^{0}$ hacia su izquierda. \\ \hline
ACTION\_RIGHT & El agente activa un actuador que le imprime una fuerza de $v_{i}^{0}$ hacia su derecha. \\ \hline
\end{tabular}
\end{center}

donde $v_{i}^{0}$ es la velocidad de deseo del agente.

\subsection{Esquema de Refuerzos}
Se decidió por un esquema de refuerzos negativos. Cada vez que el agente choca con otro agente se refuerza con un valor de $-1$. La decisión se basa en que al seguir presente la fuerza de deseo mientras el agente aprende utilizando Q-Learning no es necesario recompensarlo por avanzar correctamente.

\subsection{Exploración vs Explotación}

Inicialmente el agente tiene una probabilidad de explorar del 100\%. Esta probabilidad decrece a lo largo de la simulación, forzando a que se explote más mientras mas avanzada este la simulación. Toda exploración termina antes del 66\% de la simulación. Desde ahí en adelante el agente solo explota las decisiones aprendidas, no explora nuevas. En la \textit{figura \ref{figexploracionvsexplotacion}} se observa como decrece la probabilidad de exploración versus los pasos de simulación transcurridos.

\begin{figure}
\centering
\includegraphics{images/exploracionvsexplotacion.png}
\caption[figexploracionvsexplotacion]{\small{Probabilidad de exploración vs iteraciones de Q-Learning}}
\label{figexploracionvsexplotacion}
\end{figure}

\section{Experimentos y Resultados}

A continuación se presentan los resultados de los 4 experimentos propuestos. Los supuestos para las simulaciones son los siguientes:

\begin{itemize}
\item Se entrenó el agente durante $1000$ pasos de decisión de Q-Learning.
\item Se evalúa Q-Learning cada $200ms$ en tiempo de simulación.
\item El \textit{decay rate} (el factor que decide cuando un agente debe explorar y cuando explotar) vale $0.15$. Esto quiere decir que en $2/3$ de la simulación el agente deja de explorar por completo.
\item Sólo se otorgan refuerzos negativos cuando el agente colisiona con otro agente o con un obstáculo.
\item No se otorgan refuerzos positivos.
\item El desempeño de una simulación se mide por la razón \textit{llegadas}/\textit{colisiones}.
\item Cuando debe decidir, el agente elige la acción con utildad menos negativa.
\end{itemize}

\subsection{Formatos de Salida}

Las simulaciones arrojan resultados en formato de log, los cuáles se analizan para construir gráficos para dos métricas distintas:

\begin{itemize}
\item Desempeño: Representa la cantidad de llegadas con éxito contra la cantidad de colisiones que sufrió un agente.
\item Velocidad: Representa la velocidad a la que se trasladó el agente durante el intervalo de tiempo entre decisiones de Q-Learning.
\end{itemize}

Se imprime la matriz-Q que resultó del experimento para poder utilizarla en futuras simulaciones. El formato de la matriz es algo difícil de analizar por lo que los resultados que aquí se presentan se acompaña las matrices en un formato más ordenado.

El estado de la matriz-Q debe interpretarse de izquierda a derecha, es decir que el primer valor del vector corresponde al sensor de la derecha y el último al último sensor de la izquierda.

Los archivos de log tienen el siguiente formato:

\begin{figure}
\centering
\includegraphics{images/formatosim.png}
\caption[Sensores]{\small{Formato de archivo \textit{log} analizado.}}
\label{figFormatoSim}
\end{figure}

\subsection{Experimento 1}

El Experimento 1 consiste en dos agentes enfrentados. La posición inicial del agente entrenado será  aleatoria alrededor de la línea que pasa por el obstáculo, de modo que lo eluda a  veces por derecha y a veces por izquierda. Se estudiarán tres configuraciones diferentes:

\begin{itemize}

\item Experimento 1.1 - Un sistema de un agente con un obstáculo (un agente quieto) en la línea del target. 

\item Experimento 1.2 - Ídem pero la velocidad del agente es duplicada.

\item Experimento 1.3 - El obstáculo ahora es móvil. Es un agente que se mueve con el modelo de fuerza social reducida (solamente tiene fuerza de deseo y de contacto). El target del obstáculo queda directamente detrás del agente que aprende con \textit{Q-Learning}.
\end{itemize}

En la \textit{figura \ref{figExp1}} se detalla un esquema del experimento. Una imagen del simulador corriendo esta simulación se detalla en la \textit{figura \ref{figExp1Screen}}.

\begin{figure}
\centering
\includegraphics{images/esquemas/ej1.png}
\caption[Exp 1]{\small{Experimento 1 - Dos agentes encontrados.}}
\label{figExp1}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=14cm]{images/esquemas/ej1screen.png}
\caption[Exp 1 Screenshot]{\small{Experimento 1 - Imagen del Simulador. El punto rojo es el agente entrenando. El área de creación de agentes es \textit{spawn1}. El agente inmóvil es el punto de color ocre y el area de destino es \textit{exit1}.}}
\label{figExp1Screen}
\end{figure}

\subsubsection{Resultados}

El Experimento 1.1 es una prueba simple de la capacidad del simulador. En los dos primeros experimentos propuestos el agente que aprende se comporta como se esperaba y tiene una desempeño positivo. Las \textit{figuras \ref{figExpRes11}, \ref{figExpRes12}, \ref{figExpRes13}} son los gráficos de desempeño de los experimentos. La línea azul representa las llegadas con éxito, y la roja las colisiones.

\begin{figure}
\centering
\includegraphics{images/resultados/ej1_1.png}
\caption[Exp 1.1 Res]{\small{Experimento 1.1 - Desempeño}}
\label{figExpRes11}
\end{figure}

\begin{figure}
\centering
\includegraphics{images/resultados/ej1_2.png}
\caption[Exp 1.2 Res]{\small{Experimento 1.2 - Desempeño}}
\label{figExpRes12}
\end{figure}

\begin{figure}
\centering
\includegraphics{images/resultados/ej1_3.png}
\caption[Exp 1.3 Res]{\small{Experimento 1.3 - Desempeño}}
\label{figExpRes13}
\end{figure}

El análisis de los gráficos de desempeño revela importantes consecuencias del comportamiento del agente que aprende.
\begin{itemize}

\item En el Experimento 1.1 el desempeño experimenta una mejoría notable a partir de la mitad de la simulación. Esto es causado por la finalización de la etapa de exploración, deviniendo en una menor cantidad de movimientos aleatorios y el aprovechamiento de la estrategia aprendida (\textit{figura \ref{figExpRes11}}).

\item En el Experimento 1.2 (\textit{figura \ref{figExpRes12}}) se nota claramente el punto de inflexión entre exploración y explotación. La diferencia con el Experimento 1.1 es la velocidad del agente, que es el doble. Esto efectivamente duplica la dificultad del agente para llegar con éxito a destino.

\end{itemize}

En las \textit{matrices-Q}, (\textit{cuadros \ref{qmatrix11}, \ref{qmatrix12}, \ref{qmatrix13}}) se puede ver que los valores de utilidad son los esperados en casi todos los casos. Recordar que el primer sensor es el que está más a la derecha del agente, es decir, el que tiene un ángulo 0 con el campo de visión. Las siguientes posiciones del vector representan los vectores a medida que el ángulo incrementa entre el sensor y el campo de visión. Otro elemento a tener en cuenta es que los sensores no se distribuyen equivalentemente, si no mas bien
con una mayor concentración en los 4 sensores centrales.

En el Experimento 1.3 (\textit{figura \ref{figExpRes13}}) el desempeño es peor debido a que es más difícil aprender enfrentado a un obstáculo móvil, que intercepta al agente en distintos lugares.

\begin{table}
\begin{center}
\begin{tabular}{ | p{3cm} | c | c | c | c |}

\hline
Sensado                     &  LEFT & RIGHT & NONE  & BACK \\ \hline
0,0, 0,0, 1,0, 0,0 & 0.000000  &  \textbf{1.836658}   &  0.000000   & 0.000000  \\ \hline
0,0, 0,0, 0,0, 0,0 & 1.338391   & 1.263650   &  \textbf{1.616758}   & 1.296270  \\ \hline
0,1, 0,0, 0,0, 0,0 & -0.190000  &  2.044788  &   0.976724  &  \textbf{3.995214} \\ \hline
1,0, 0,0, 0,0, 0,0 & 0.000000   & 2.606554   &  3.533919   & \textbf{5.292406} \\ \hline
0,0, 0,1, 1,0, 0,0 & 0.107069   & \textbf{5.984513}   &  0.000000   & 0.000000 \\ \hline
0,0, 1,0, 0,0, 0,0 & -0.100000  &  0.384343  &   0.302075  &  \textbf{2.437552} \\ \hline
0,0, 0,1, 0,0, 0,0 & 0.782559   & \textbf{2.040882}  &   0.947097   & 1.897630 \\ \hline

\end{tabular}
\caption{\small{Matriz de Q - Experimento 1.1}}
\label{qmatrix11}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{ | p{3cm} | c | c | c | c |}
\hline
Sensado                     &  LEFT & RIGHT & NONE  & BACK \\ \hline
0,0, 1,0, 0,0, 0,0 & -0.271000  &  -0.091080   &  -0.294573  &  \textbf{1.124596} \\ \hline
0,0, 0,0, 0,0, 1,0 & \textbf{6.092650}   & 0.437323    & 0.458220   & 0.000000 \\ \hline
0,0, 0,2, 2,0, 0,0 & 0.674703   & 1.052398    & 0.990000   & \textbf{8.502963} \\ \hline
0,0, 0,0, 0,0, 0,1 & 0.000000   & 0.000000    & \textbf{6.590362}   & 1.190996 \\ \hline
0,0, 0,0, 1,0, 0,0 & \textbf{6.790741}   & 1.412947    & 2.451154   & 1.753274 \\ \hline
0,1, 0,0, 0,0, 0,0 & 0.000000   & 1.458494    & 1.824140   & \textbf{3.999286} \\ \hline
0,0, 0,1, 0,0, 0,0 & \textbf{2.799878}   & 0.278172    & 0.562676   & 0.514212 \\ \hline
0,0, 0,1, 1,0, 0,0 & \textbf{7.952153}   & 1.125422    & 0.877281   & 0.000000 \\ \hline
1,0, 0,0, 0,0, 0,0 & 0.000000   & -0.100000    & \textbf{3.409122}   & 0.000000 \\ \hline
0,0, 0,2, 0,0, 0,0 & 0.527716   & \textbf{5.445163}    & 5.234501   & 5.248148 \\ \hline
0,0, 0,0, 2,0, 0,0 & 2.458329   & 4.871357    & 3.841234   & \textbf{5.929559} \\ \hline
0,0, 0,0, 0,0, 0,0 & 2.983337   & 3.061287    & \textbf{3.950212}   & 2.933851 \\ \hline
0,0, 0,0, 0,1, 0,0 & \textbf{5.547692}   & 0.000000    & -0.100000   & 0.455750 \\ \hline

\end{tabular}
\caption{\small{Matriz de Q - Experimento 1.2}}
\label{qmatrix12}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{ | p{3cm} | c | c | c | c |}
\hline
Sensado                     &  LEFT & RIGHT & NONE  & BACK \\ \hline
0,0, 1,0, 0,0, 0,0 &  0.000000  & 2.607660  &   2.537028  &  \textbf{4.343992} \\ \hline
0,0, 0,0, 1,0, 0,0 & 3.092437   & \textbf{3.271224}  &   3.127772  &  3.015895 \\ \hline
0,0, 0,1, 1,0, 0,0 & 0.616420   & \textbf{6.633904}  &   1.095116  &  1.609967 \\ \hline
0,1, 0,0, 0,0, 0,0 & 0.000000   & 4.580436  &   3.459806  & \textbf{4.894548} \\ \hline
1,0, 0,0, 0,0, 0,0 & 0.000000   & 4.607929  &   \textbf{5.665124}  &  4.922007 \\ \hline
0,0, 0,0, 0,0, 0,0 & 2.838395   & \textbf{3.380589}  &   3.100250  &  2.807318 \\ \hline
0,0, 0,2, 2,0, 0,0 & 1.497292   & 0.000000  &   \textbf{4.266430}  &  1.418284 \\ \hline
0,0, 0,1, 0,0, 0,0 & 2.827251   & 3.542712  &   2.995507  &  \textbf{4.254867} \\ \hline
0,0, 0,2, 0,0, 0,0 & 1.272467   & 6.238893  &   5.538935  &  \textbf{6.844985} \\ \hline
0,0, 0,0, 2,0, 0,0 & 0.000000   & \textbf{4.481721}  &   3.095885  &  2.232653 \\ \hline

\end{tabular}
\caption{\small{Matriz de Q - Experimento 1.3}}
\label{qmatrix13}
\end{center}
\end{table}


\subsection{Experimento 2}

Este experimento consiste en un pasillo largo y angosto, con un flujo de agentes moviéndose en una dirección. El pasillo tiene 3 metros de ancho y 15 metros de largo. Hay treinta peatones en un extremo, donde uno usa \textit{Q-Learning}, que tienen que llegar al otro lado.

Se experimentó con distintas densidades de agentes. Una densidad \textit{alta} ($2/m^{2}$) y otra \textit{baja} ($1/m^{2}$). Los agentes se mueven con la velocidad de deseo $v_{i}^{0}$.

Los destinos de los agentes se fueron variando en los experimentos, se estudió el caso que los agentes se mueven en lineas rectas
o hacia puntos aleatorios dentro del área de llegada. Los otros agentes que no se están entrenando usan el modelo social como 
política de navegación. Un esquema del experimento se encuentra en la \textit{figura \ref{figExp2}}. La \textit{figura \ref{figExp2Screen}} detalla al Simulador corriendo el Experimento 2. La \textit{figura \ref{figExp2ScreenClose}} se acerca sobre el área de los agentes de baja densidad mientras que la \textit{figura \ref{figExp2ScreenHigh}} muestra una alta densidad de los agentes.

En resumen se estudiarán dos variables en este esquema: la densidad y las trayectorias. Esto nos da 4 escenarios para experimentar:

\begin{itemize}
\item Experimento 2.1 - Densidad Baja con Destinos Alineados
\item Experimento 2.2 - Densidad Baja con Destinos Cruzados
\item Experimento 2.3 - Densidad Alta con Destinos Alineados
\item Experimento 2.4 - Densidad Alta con Destinos Cruzados

\end{itemize}

\begin{figure}
\centering
\includegraphics{images/esquemas/ej2.png}
\caption[Exp 2]{\small{Experimento 2 - Un corredor largo con 30 agentes}}
\label{figExp2}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=14cm]{images/esquemas/ej2screen.png}
\caption[Exp 2 Screen]{\small{Experimento 2 - Imagen del Simulador. Se observa el pasillo largo y la aréa de creación de agentes a la izquierda y el área destino a la derecha.}}
\label{figExp2Screen}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=14cm]{images/esquemas/ej2screenclose.png}
\caption[Exp 2 Screen Close]{\small{Experimento 2 - Imagen del Área de Creación. Se observa el agente de \textit{Q-Learning} con color rojo. Los agentes de color azul representan agentes que implementan el modelo de \textit{fuerza social}. La densidad es de $1/m^{2}$.}}
\label{figExp2ScreenClose}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=14cm]{images/esquemas/ej2screenhigh.png}
\caption[Exp 2 Screen High]{\small{Experimento 2 - Imagen del Area de Creación de Alta Densidad. La densidad de los agentes es de $2/m^{2}$.}}
\label{figExp2ScreenHigh}
\end{figure}



\subsubsection{Resultados}
Los resultados del Experimento 2 revelan que la forma de presentar el estado escogida resulta inadecuada cuando aumenta considerablemente la densidad de peatones. Al tener dicha densidad, un agente que opte por detenerse chocará con el agente detrás.
Debido a nuestro esquema de sensado, el agente no puede aprender de dicha situación porque sencillamente no lo percibe. Aún en los experimentos de baja densidad el resultado muestra que por más que llega un número de veces considerable, parecido al Experimento 1, la tasa de fracasos vs éxitos no cambia. El agente sigue colisionando en la misma proporción aún después de terminar la etapa de aprendizaje.

Las \textit{matrices-Q} del Experimento 2 se excluyeron del documento debido a su extensión, de cientos de estados, y difíciles de interpretar a simple vista. Sin embargo se observó que en muchos casos todas las acciones asociadas a un estado tienen utilidades muy similares o los valores de utilidad más altos no están asociados a la acción que uno esperaría. Esto sucede por varias razones:

\begin{itemize}
	\item El agente se encuentra atrapado entre otros agentes y no existe una acción que lo haga salir de esa posición, tome la acción que tome choca con otro agente en los costados y por eso se lo recompensa de manera negativa.
	\item El agente se encuentra atrapado entre otros agentes, si decide frenar sucede muy seguido que un agente más retrasado lo choca y por eso se lo recompensa de manera negativa también. Por más que el agente continúe moviéndose a la velocidad promedio del grupo, existen colisiones por pequeños destellos de velocidad dentro del grupo. Debido al modelo de fuerza social, cuando dos agentes estan muy cerca se producen fuerzas que ``empujan'' con gran aceleración a alguno de estos agentes. 
	\item Cuando el agente aplica una fuerza correctiva, no puede moderar la magnitud de la misma, lo que causa que al tratar de eludir un obstáculo termine golpeando a otro.
	\item Los otros agentes también eluden al agente debido al modelo de fuerza social. Este reflejo salva malas decisiones que tomó el agente y que no se castigan.
\end{itemize}




\begin{figure}
\centering
\includegraphics{images/resultados/ej2_1.png}
\caption[Exp 2.1 Res]{\small{Experimento 2.1 - Desempeño}}
\label{figExpRes21}
\end{figure}

\begin{figure}
\centering
\includegraphics{images/resultados/ej2_2.png}
\caption[Exp 2.2 Res]{\small{Experimento 2.2 - Desempeño}}
\label{figExpRes22}
\end{figure}

\begin{figure}
\centering
\includegraphics{images/resultados/ej2_3.png}
\caption[Exp 2.3 Res]{\small{Experimento 2.3 - Desempeño}}
\label{figExpRes23}
\end{figure}

\begin{figure}
\centering
\includegraphics{images/resultados/ej2_4.png}
\caption[Exp 2.4 Res]{\small{Experimento 2.4 - Desempeño}}
\label{figExpRes24}
\end{figure}


\subsection{Experimento 3}

El Experimento 3 es una modificación del Ejercicio 1, donde se reemplazó el osbtáculo con N agentes enfrentados. Para el aprendizaje se utiliza uno que aprende con un ``bosque'' de obstáculos (agentes fijos) en un arreglo desordenado. En este caso usar las 3 variantes de aprendizaje análogas a las del Experimento 1.

\begin{itemize}
	\item Agente con velocidad de deseo normal ($ v_{i}^{0} = 1,2 m/s$) y obstáculos fijos.
	\item Agente con el doble de velocidad de deseo y obstáculos fijos.
	\item Agente con velocidad de deseo normal y los obstáculos son ahora agentes móviles que se mueven en bloque todos con la misma velocidad de deseo en sentido opuesto al del agente que aprende.
\end{itemize}

Un esquema representativo del experimento se encuentra detallado en la \textit{figura \ref{figExp3}}. La \textit{figura \ref{figExp3Screen}} es una imagen de este llevándose a cabo. Se puede apreciar que el bosque de agentes es bastante denso pero tiene caminos posibles por donde el agente entrenado con \textit{Q-Learning} puede transitar. Este es de color rojo mientras que los obstáculos son de color ocre.

\begin{figure}
\centering
\includegraphics{images/esquemas/ej3.png}
\caption[Exp 3]{\small{Experimento 3 - Un agente enfrentado a un bosque de agentes}}
\label{figExp3}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=14cm]{images/esquemas/ej3screen.png}
\caption[Exp 3]{\small{Experimento 3 - Imagen del Simulador}}
\label{figExp3Screen}
\end{figure}

\subsubsection{Resultados}

El Experimento 3.1 arroja resultados que no son positivos ni esperados. Uno esperaría encontrar algún tipo de impacto en las colisiones vs las llegadas cuando el agente deja de explorar y explota la política aprendida. Sin embargo esto no sucede y las tendencias parecen mantenerse a lo largo de la simulación. Es valioso rescatar que aún así el agente logró llegar a un ritmo constante a destino.

En el Experimento 3.2 podemos concluir que el agente prácticamente no llegó a destino. En realidad el agente logró llegar apenas unas 4 veces. Esto nos conduce a pensar que el alcance de visión del agente no es el adecuado para la velocidad que tiene. La velocidad es demasiado alta y no le permite actuar a tiempo y evitar la colisión.

El Experimento 3.3 presentó un resultado inesperado y curioso. En vista de los experimentos anteriores, era esperable que el agente obtuviera malos resultados dada la dificultad agregada a los Experimentos o a lo sumo tendría un desempeño igual o peor que el Experimento 3.1. Al ver los gráficos (\textit{figura \ref{figExpRes32}}) vemos que sorprendentemente le fue mejor. Al consultar los replays aprendimos que la razón de esto es que los agentes obstáculos también eludían al agente. Si bien esto también debía ocurrir en el Experimento 2, en este los agentes obstáculos tenían más lugar para maniobrar y lo hacían de una manera mucho más efectiva.

\begin{figure}
\centering
\includegraphics{images/resultados/ej3_1.png}
\caption[Exp 3.1 Res]{\small{Experimento 3.1 - Desempeño}}
\label{figExpRes31}
\end{figure}

\begin{figure}
\centering
\includegraphics{images/resultados/ej3_2.png}
\caption[Exp 3.2 Res]{\small{Experimento 3.2 - Desempeño}}
\label{figExpRes32}
\end{figure}

\begin{figure}
\centering
\includegraphics{images/resultados/ej3_3.png}
\caption[Exp 3.3 Res]{\small{Experimento 3.3 - Desempeño}}
\label{figExpRes33}
\end{figure}

\subsection{Experimento 4}

El Experimento 4 se propone evaluar una política de navegación previamente obtenida en un entorno más aproximado a la realidad de transeúntes. El entorno es un cruce de calles perpendiculares, con agentes avanzando en ambos sentidos en cada calle. Se usarán las políticas de aprendizaje obtenidas de los Experimentos 3. Un esquema del experimento está descrito en la \textit{figura \ref{figExp4}}. La \textit{figura \ref{figExp4Screen}} muestra al simulador corriendo el experimento mientras que la \textit{figura \ref{figExp4ScreenClose}} muestra con mayor detalle los agentes. Los experimentos llevados a cabo son los siguientes:

\begin{itemize}
	\item Experimento 4.1 - Experimento 4 con la política obtenida del Experimento 3.1
	\item Experimento 4.2 - Experimento 4 con la política obtenida del Experimento 4.2
	\item Experimento 4.3 - Experimento 4 con la política obtenida del Experimento 4.3
\end{itemize}

\begin{figure}
\centering
\includegraphics{images/esquemas/ej4.png}
\caption[Exp 3]{\small{Experimento 4 - Cruce de calles transitadas}}
\label{figExp4}
\end{figure}



\begin{figure}
\centering
\includegraphics[width=14cm]{images/esquemas/ej4screen.png}
\caption[Exp 4]{\small{Experimento 4 - Se aprecian las áreas de llegada y de salida en todos los extremos de los pasillos.}}
\label{figExp4Screen}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=14cm]{images/esquemas/ej4screenclose.png}
\caption[Exp 4 Close]{\small{Experimento 4 - Area de Llegada 3. Se observa el agente ya entrenado con color verde. Los otros agentes usan el modelo de \textit{fuerza social}.}}
\label{figExp4ScreenClose}
\end{figure}

\subsubsection{Resultados}

Los resultados de este experimento están decididamente relacionados con los anteriores y otra vez sorprenden. El experimento que peor desempeño tuvo, el Experimento 4.1 (\textit{figura \ref{figExpRes41}}), era el único que tenía una \textit{Matriz de Q} que contenía algún tipo de aprendizaje.

 El Experimento 4.2 y el Experimento 4.3 tuvieron un mejor desempeño. Esto a pesar que en el Experimento 3.2 el agente llegó una cantidad despreciable de veces, y en el Experimento 3.3 el agente fue salvado en su mayoría por los otros agentes que lo eludían. Esto nos conduce nuevamente a pensar en los otros agentes y cómo estos eluden al agente entrenado. Se ve que si el agente no aprendió nada, es inclusive más beneficioso para los otros que si hubiera aprendido a eludir él mismo (caso Experimento 4.1). 

 Una razón de esto es que si los dos quieren eludirse, es posible que los dos decidan eludir por el mismo lado y chocar, mientras que siempre es más fácil eludir a algo inmóvil. Por más que hubo una leve mejora en el Experimento 4.3 respecto del Experimento 4.2, es importante recalcar que la diferencia es mínima en cantidad de llegadas exitosas.


\begin{figure}
\centering
\includegraphics{images/resultados/ej4_1.png}
\caption[Exp 4.1 Res]{\small{Experimento 4.1 - Desempeño}}
\label{figExpRes41}
\end{figure}

\begin{figure}
\centering
\includegraphics{images/resultados/ej4_2.png}
\caption[Exp 4.2 Res]{\small{Experimento 4.2 - Desempeño}}
\label{figExpRes42}
\end{figure}

\begin{figure}
\centering
\includegraphics{images/resultados/ej4_3.png}
\caption[Exp 3.3 Res]{\small{Experimento 4.3 - Desempeño}}
\label{figExpRes43}
\end{figure}

\section{Conclusión}

Se modificó el modelo de fuerza social propuesto por Helbing \cite{helbing:panic} para incluir elementos del aprendizaje por refuerzo. 
Se mantuvieron las fuerzas de deseo y granular pero se reemplazó la fuerza social por un ajuste de dirección que 
surge de la inclusión del aprendizaje por refuerzo.
Se implementó un simulador que tiene la capacidad para manejar distintos métodos de movimiento, entre ellos el 
modelo de fuerza social.

El esquema propuesto de sensores y actuadores como reemplazo de la fuerza social, resulta adecuado en los casos de baja densidad de obstáculos, como en el experimento 1, 
para otros casos de mayor complejidad como los de los experimentos 3 y 4 este esquema no es suficiente para navegar eficientemente (es decir sin colisiones).

Otros posibles enfoques para intentar mejorar esta limitación sería:
\begin{itemize}
	\item Aumentar el número de sensores y sus valores.
	\item Redefinir el riesgo de choque.
	\item Un modelo donde el agente tenga una cantidad de sensores similar, agrupados en un ángulo de menor amplitud pero con un rango de visión considerablemente mayor como por ejemplo 30m. Los sensores incluso podrían funcionar como rayos en vez de triángulos ya que lo que se intenta evitar es chocar con obstáculos de gran tamaño y para ello no es necesario tener noción de que hay entre cada sensor sino que es suficiente con tener una visión general de los obstáculos en distancia.
\end{itemize}


\subsection{Modelo Propuesto}

La idea atrás de este modelo es que el maniobrar en situaciones de alta densidad de obstáculos sea manejado por el modelo de fuerza social pero que el agente tenga la suficiente inteligencia para eludir obstáculos que no le reportan un perjuicio inmediatamente.

Para lograr esto los sensores de Q-Learning solo podrían sensar obstáculos mas allá del rango inmediato del agente, es decir a partir de los 2 o 3 metros.
El esquema de refuerzo no contemplaría refuerzos negativos sino que recompensaría al agente cuando sus sensores detecten menos obstáculos que en una iteración anterior. Se debería dar mayor importancia al modelo de fuerza social cuando hay obstáculos cerca ya que siempre será más importante la resolución inmediata de conflictos.

\begin{figure}
\centering
\includegraphics[width=6cm]{images/modelo_sensores.png}
\caption[Modelo Sensores]{\small{Modelo Propuesto - La sección verde es la detectada por \textit{Q-Learning}. La parte sin sombrear es para ser resuelta por un modelo de fuerza social.}}.
\label{figModeloSensores}
\end{figure}


\subsection{Posibilidad de crear entornos usando una herramienta gráfica}

En este momento el simulador solo acepta entornos creados a partir de un archivo JSON. Sería de gran utilidad que éste permitiera la creación de entornos y agentes utilizando el mouse y arrastrando estos elementos al área de simulación.
Idealmente se permitiría grabar los entornos en el mismo formato JSON para luego poder abrirlos y correrlos en otro momento.

\subsection{Posibilidad de exportar simulaciones en formato mp4 o AVI}

Las simulaciones creadas por este simulador no pueden verse en ningún otro lado, es decir, se requiere del simulador para poder reproducir los replays. Sería conveniente para el usuario poder exportar los replays a algún formato no propietario de vídeo para poder analizarlos cuando le resulte cómodo.
El problema a resolver para agregar esta funcionalidad es el tamanño que tendrían los vídeos si se guardara cuadro por cuadro todos los estados de la simulación por lo que habría que encontrar un buen compromiso entre calidad y tamaño.

\subsection{Mapeo de agentes en un KD-Tree }

Lo más costoso al realizar las simulaciones es el sensado y el cálculo de fuerza social. Ya se implementaron optimizaciones que descartan agentes y obstáculos que se encuentran mas allá de una distancia prudencial pero una forma adicional de optimizar este cálculo sería agregar los agentes a un árbol KD\footnote{En ciencias de la computación, un Árbol KD (abreviatura de árbol k-dimensional) es una estructura de datos de particionado del espacio que organiza los puntos en un espacio euclídeo de k dimensiones. Los árboles KD son un caso especial de los árboles BSP.
Un árbol KD emplea sólo planos perpendiculares a uno de los ejes del sistema de coordenadas. Esto difiere de los árboles BSP, donde los planos pueden ser arbitrarios. Además, todos los nodos de un árbol KD, desde el nodo raíz hasta los nodos hoja, almacenan un punto. Mientras tanto, en los árboles BSP son las hojas los únicos nodos que contienen puntos (u otras primitivas geométricas). Como consecuencia, cada plano debe pasar a través de uno de los puntos del árbol KD} para obtener más rápidamente los vecinos inmediatos y de esta manera acelerar la simulación.

\subsection{Elección del lenguaje y plataforma}

Teniendo en cuenta ciertas dificultades que se encontraron durante el desarrollo, principalmente en el área de GUI, 
consideramos que \textit{Java}, si bien es un lenguaje muy flexible, puede no haber sido la mejor elección sobre la cual 
construir el proyecto.
Una mejor elección podría ser la utilización de un motor de video juegos para poder aprovechar la sincronización de 
threads y las vastas librerías de vectores y física que estos motores poseen. Entre ellos los que más llaman la atención 
son:

\begin{itemize}
\item Cocos2d: (http://cocos2d.org/) es una librería de Python opensource que existe desde febrero de 2008. Su principal 
ventaja sobre la utilización de JAVA es como ya se mencionó, que este motor permite un manejo mas cómodo de la 
graficación. Su otra ventaja es que al estar basada sobre OpenGL provee una interfaz mas fluida al usuario.
\item Unity3D: (http://unity3d.com/) es un motor de vídeo juegos con soporte nativo multiplataforma. Si bien no es 
opensource como Cocos2d, provee con un entorno de edición que facilita tareas típicas que acelera mucho el 
desarrollo. En nuestra opinión esta es la mejor de las 2 opciones.
\end{itemize}

\bibliographystyle{abbrv}
\bibliography{01}


\end{document}
